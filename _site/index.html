<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Python/Ml/NLP </title>

<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">





<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Python/Ml/NLP">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="/index.html">
<meta property="og:site_name" content="Scott Ni">





<link rel="canonical" href="/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Scott Ni Feed">
<link rel="author" href="https://plus.google.com/u/0/105487198769272638847?rel=author">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/apple-touch-icon-144x144-precomposed.png">


<style type="text/css">body {background-image:url(/images/ps_neutral.png);}</style>





<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?a83a943ad8eb0be645a1efefca583ea5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body id="post-index" class="feature" itemscope itemtype="http://schema.org/WebPage">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation" itemscope itemtype="http://schema.org/SiteNavigationElement">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href=""><i class="icon-home icon-large" icon-large></i>Home</a></li>

		<li>
			<a href="#"><i class="icon-user icon-large"></i>About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/np.jpg" alt="Ni Peng photo" class="author-photo">
					<h4>Ni Peng </h4>
					<p></p>
				</li>
				<li><a href="/about/"><i class="icon-info-sign icon-large"></i>Learn More</a></li>
				<li>
					<a href="mailto:nipengmath@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				<li>
					<a href="https://plus.google.com/u/0/105487198769272638847"><i class="icon-google-plus"></i> Google+</a>
				</li>
				
				<li>
					<a href="http://github.com/nipengadmaster"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
				

			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#"><i class="icon-book icon-large"></i>Posts</a>
			<ul class="dl-submenu">
				<li><a href="/archive/"><i class="icon-bookmark icon-large"></i>All Posts</a></li>
								<li><a href="/tags/"><i class="icon-tags icon-large"></i>All Tags</a></li>
								<li><a href="/categories/"><i class="icon-folder-close icon-large"></i>All Categories</a></li>

				<li>
					<a href="#"><i class="icon-folder-open icon-large"></i>Catagory</a>
					<ul class="dl-submenu">
						
						<li>
							<a href = "/categories/#杂记">杂记(1)</a>
						</li>
						
						<li>
							<a href = "/categories/#笔记">笔记(7)</a>
						</li>
						
					</ul>
				</li>
			</ul>
		</li>
		<li>
					<a href="#"><i class="icon-tasks icon-large"></i>Projects</a>
				</li>
				<li>
					<a href="#"><i class="icon-camera-retro icon-large"></i>Gallery</a>
				</li>



		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
  	
		<div class="entry-image"><img src="/images/feiwu.jpg" alt="Python/Ml/NLP" itemprop="image">
	
    
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Scott Ni</h1>
      <h2 itemprop="name">Python/Ml/NLP</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main" itemprop="mainContentOfPage" itemscope itemtype="http://schema.org/Blog">
  
<article class="hentry" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-11-20T00:00:00+08:00" itemprop="datePublished"><a href="/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0">November 20, 2013</a></time></span><span class="author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name" class="fn"><a href="/about" title="About Ni Peng" itemprop="url">Ni Peng</a></span></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title" itemprop="name"><a href="/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0" rel="bookmark" title="Mlapp笔记" itemprop="url">Mlapp笔记</a></h1>
    
  </header>
  <div class="entry-content" itemprop="description">
	  
			<h2>MLAPP笔记</h2>

<h3>1. Introduction</h3>

<ol>
<li>机器学习分为两类

<ul>
<li>预测性/有监督学习--分类/模式识别/回归/<code>有序回归</code></li>
<li>描述性/无监督学习--知识挖掘</li>
<li>强化学习</li>
</ul>
</li>
<li>分类：二分类，多分类，多标签分类。</li>
<li>SmartASS, Meta2010, CTR预估

<blockquote><p>google</p></blockquote></li>
<li>非监督学习：

<ol>
<li>讨论p(xi|theta)而不是p(yi|xi, theta)</li>
<li>xi是特征词向量，所有需要构造出多维概率模型，对于监督学习，yi是单个变量，因此对多数监督学习，可以应用单变量的概率模型来处理，因此可以简化问题。</li>
</ol>
</li>
<li>非监督学习是人类和动物学习所特有的。应用比监督学习更广泛，因为不需要已标签数据。</li>
<li>标签数据不仅难得到，而且包含相对较少的信息。</li>
<li>model based clustering, ad hoc algorithm

<blockquote><p>google
Berkhin 2006 e-commerce cluster users</p></blockquote></li>
<li>LSI是PCA的变体，因为SVD</li>
<li>ICA是PCA的变体</li>
<li></li>
</ol>


	  
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-11-18T00:00:00+08:00" itemprop="datePublished"><a href="/%E7%AC%94%E8%AE%B0/ProGit%E7%AC%94%E8%AE%B0">November 18, 2013</a></time></span><span class="author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name" class="fn"><a href="/about" title="About Ni Peng" itemprop="url">Ni Peng</a></span></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title" itemprop="name"><a href="/%E7%AC%94%E8%AE%B0/ProGit%E7%AC%94%E8%AE%B0" rel="bookmark" title="Progit笔记" itemprop="url">Progit笔记</a></h1>
    
  </header>
  <div class="entry-content" itemprop="description">
	  
			<h2>《Progit中文版》</h2>

<h3>1. 起步</h3>

<ol>
<li>git不保存前后变化的差异，而是直接快照</li>
<li>使用SHA-1算法计算数据的校验和</li>
<li>三种状态：

<ul>
<li>已提交(commited)</li>
<li>已修改(modified)</li>
<li>已暂存(staged)</li>
</ul>
</li>
<li><p>设置git默认编辑器</p>

<pre><code> git config --global core.editor emacs
</code></pre></li>
<li>使用帮助

<ul>
<li>git help <verb></li>
<li>git <verb> --help</li>
<li>man git <verb></li>
</ul>
</li>
</ol>


<h3>2. Git 基础</h3>

<ol>
<li><p>查看文件内容</p>

<pre><code> cat file1
</code></pre></li>
<li><p>git diff：查看<code>已修改未暂存</code>文件的修改</p>

<p>git diff --cached：查看<code>已暂存未提交</code>文件的修改</p></li>
<li>git rm -f： 删除修改过的，并且已暂存的文件。force。</li>
<li>git rm --cached: 删除版本库中文件，保留工作目录中文件。</li>
<li>递归方式匹配文件，删除，查找

<blockquote><p>fix me</p></blockquote></li>
<li>git mv file1 file2: git 中对文件重命名。</li>
<li>git log -p -2:

<ul>
<li>-p: 显示每次提交内容差异</li>
<li>-2: 最近两次提交</li>
</ul>
</li>
<li>git log --stat: 显示摘要，增改的行数</li>
<li>git log:

<ul>
<li>--pretty=online 单行显示每次提交</li>
<li>--since=2.weeks --after显示指定时间之后的提交</li>
<li>--until, --before 显示指定时间之前的提交</li>
</ul>
</li>
<li>git commit --amend: 修改最后一次提交</li>
<li>任何提交给git的数据都可以恢复，即便在已经删除的分支中的提交。可能失去的数据仅限于没有commit过的数据。</li>
<li>git remote -v：查看克隆的地址</li>
<li>git remote add pb git://*.git</li>
<li>git fetch: 只是将远程数据拉到本地，并不自动合并到当前分支。</li>
<li>git pull = fetch + merge</li>
<li>git remote show origin: 查看远程仓库信息</li>
<li>git remote rename name1 name2： 对远程分支重命名</li>
<li>git remote rm pb: 删除远程分支</li>
<li>git tag: 列出现有标签</li>
<li>git tag -l "v1.4.2*": 列出满足匹配条件的标签</li>
<li>git push origin v1.5: git push不会把标签推到远程，只能手工推送。</li>
<li>git push origin --tags: 推送所有的标签到远程。</li>
<li>切换分支的时候最好保持一个清洁的工作区域。</li>
<li> stashing  ammending</li>
</ol>


<h3>3. Git分支</h3>

<ol>
<li>git branch -d hotfix: 删除分支</li>
<li>git mergetool: 图形界面的工具来解决问题。</li>
<li>git branch -v: 查看各分支最后一次提交</li>
<li>git branch --no-merged: 查看尚未合并的分支</li>
<li>git branch -D feature: 删除未合并的分支</li>
<li>git push origin 本地分支:远程分支   将本地分支推送到远程分支，可重命名</li>
<li><p>fetch抓回来的新的远程分支后，仍然不能在本地编辑，所有要checkout一个新的分支，对应远程分支</p>

<p>git checkout -b myserver origin/server</p></li>
<li><p>删除远程分支：git push origin :serverfix  表示将空分支推送到远程分支，即删除。</p></li>
<li>git rebase 衍合，在一个分支上进行的操作，在另一个分支上重演。</li>
<li>git rebase 主分支 特性分支：检出特性分支，在主分支上重演。</li>
<li><code>永远不要衍合那些推送到公共版本库的更新</code></li>
</ol>


<h3>4. 服务上的git</h3>

<ol>
<li>使用authorized_keys方法给用户授权。</li>
</ol>


<h3>5. 分布式git</h3>

<ol>
<li>不要在更新中提交多余的空白符</li>
<li>将每次提交限制在一次逻辑单元，适当分解为多次小提交。</li>
<li>git add --patch

<blockquote><p>fixme</p></blockquote></li>
<li><p><code>学习提交说明</code></p>

<pre><code> git clone git://git.kernel.org/pub/scm/git/git/git
</code></pre></li>
<li>git diff master…contrib: 查看特性分支和它同master分支的共同祖先之间的差异。</li>
<li>先将代码合并到临时特性分支，等到该分支稳定下来并通过测试，再并入develop分支，然后，经时间检验，代码可以正常工作相当长的一段时间，再并入主分支发布。</li>
<li>大项目合并流程：

<ul>
<li>master 发布</li>
<li>next 用于合并基本稳定特性</li>
<li>pu  用于合并仍需改进特性</li>
<li>maint 用于出错维护</li>
</ul>
</li>
<li>git shortlog 可以方便快捷的制作一份修改日志。</li>
</ol>


<h3>6.Git工具</h3>

<ol>
<li>git reflog：查看引用日志</li>
<li>git show HEAD@{5}: 查看HEAD在第5次前的commit记录</li>
<li>git show master@{yesterday} 查看一定时间前分支在哪</li>
<li>git show HEAD^: 指HEAD的父提交</li>
<li>两点：

<ul>
<li>git log master..experiemnt：指所有“可以从experiemnt分支中获得而不能从master中获得的提交”</li>
<li>git log refA..refB
&lt;=> git log ^refA refB
&lt;=> git log refB --not refA</li>
</ul>
</li>
<li>三点：指定被两个引用中的一个包含但不被两者<code>同时</code>包含的分支

<ul>
<li>git log master…experiment</li>
<li>git log --left-right master…experiment： 显示提交在哪个分支</li>
</ul>
</li>
<li>git stash: 往堆栈中推送一个新的储藏。</li>
<li>git stash list: 列出暂存的东西</li>
<li>git stash apply: 应用刚存的东西</li>
<li>git statsh apply stash@2: 应用第2个储藏</li>
<li><p>apply只尝试应用储藏的工作，并不删除储藏的内容。如果应用完之后删除，则可以运行</p>

<pre><code>git stash pop
</code></pre></li>
<li>git stash branch mybranch: 创建一个新的分支，检出你储藏的工作提交，重新应用你的工作。如果成功，将会丢弃储藏。</li>
<li>git commit --amend 改写最近一次提交</li>
<li>git reset HEAD^: 重置提交</li>
<li><p>从所有提交中删除一个文件,<code>慎重使用</code></p>

<pre><code>git filter-branch --tree-filter 'rm -f *~' HEAD
</code></pre>

<p> --tree-filter: 每次检出项目时，先执行指定的命令，然后重新提交结果。</p></li>
<li>git blame -L 12,22 mytest.py: 查看每一行代码的最后提交者及日期，-L 指定行数。</li>
<li><p>bisect会在你的提交历史中二分查找快速定位错误代码引入时间。</p>

<pre><code>git bisect start        # 设定查找开始
git bisect bad          # 设定当前位置为bad
git bisect good v1.0    # 设定好代码的位置
[*******]               # 到底good和bad提交的中间位置
# 重新运行代码，查看是对是错, 并标记good/bad
git bisect good
[*******]               # 到达下一个二分查找位置
…
</code></pre></li>
</ol>


<h3>7. 自定义Git</h3>

<ol>
<li>git config --global core.editor emacs 配置默认编辑器</li>
<li>git config --global color.ui true  git中的着色</li>
<li>外部合并工具：P4Merge</li>
<li><p>格式化和空白符</p>

<pre><code> git config --global core.autocrlf 
</code></pre>

<h3>8.git 与其他系统</h3></li>
</ol>


<h3>9.Git内部原理</h3>

	  
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-28T00:00:00+08:00" itemprop="datePublished"><a href="/%E7%AC%94%E8%AE%B0/StanfordCoreNlp-java%E4%BB%A3%E7%A0%81-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">October 28, 2013</a></time></span><span class="author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name" class="fn"><a href="/about" title="About Ni Peng" itemprop="url">Ni Peng</a></span></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title" itemprop="name"><a href="/%E7%AC%94%E8%AE%B0/StanfordCoreNlp-java%E4%BB%A3%E7%A0%81-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0" rel="bookmark" title="Stanfordcorenlp Java代码 学习笔记" itemprop="url">Stanfordcorenlp Java代码 学习笔记</a></h1>
    
  </header>
  <div class="entry-content" itemprop="description">
	  
			<h2>stanford-corenlp-3.2.0b.jar</h2>

<h3>1. edu.stanford.nlp.pipline</h3>

<h4>1.1 StanfordCoreNLP.class</h4>

<p>这是一个管道，输入：字符串，输出：分析好的语言学的形式。</p>

<p>类被设计成可以应用多个标注器（Annocators）。</p>

<p>首先增加一个标注器来构造管道，然后输入你想标注的东西，得到已经标注好的形式。</p>

<p>比如：</p>

<pre><code>java edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit -file document.txt
</code></pre>

<p>API的主要入口是StanfordCoreNLP.process()。</p>

<h4>1.2 Annotation.class</h4>

<p>super:</p>

<pre><code>在Java中，有时还会遇到子类中的成员变量或方法与超类（有时也称父类）中的成员变量或方法同名，因为子类中的成员变量或方法名优先级高，所以子类中的同名成员变量和方法就隐藏了超类的成员变量或方法，但是我们如果想要使用超类中的这个成员变量或方法，此使就需要用到super，请看下面的类。
</code></pre>

<p>final：</p>

<pre><code>根据程序上下文环境，Java关键字final有“这是无法改变的”或者“终态的”含义，它可以修饰非抽象类、非抽象类成员方法和变量。你可能出于两种理解而需要阻止改变：设计或效率。 
final类不能被继承，没有子类，final类中的方法默认是final的。 
final方法不能被子类的方法覆盖，但可以被继承。 
final成员变量表示常量，只能被赋值一次，赋值后值不再改变。 
final不能用于修饰构造方法。 
</code></pre>

<h5>注释</h5>

<p>  private static final long serialVersionUID = 1L;  <code>私有静态的常量</code></p>

<p>  public Annotation(Annotation map)     <code>构造函数</code></p>

<p>  public Annotation copy()  <code>覆盖copy</code></p>

<p>  public Annotation(String text)  <code>构造函数</code></p>

<p>  public String toString()  <code>覆盖copy</code></p>

<p>  public Annotation(List<CoreMap> sentences) <code>构造函数</code></p>

<h3>2. edu.stanford.nlp.sentiment</h3>

<h4>2.1 CollapsUnaryTransformer.java</h4>

<p>转化器分解一元节点组成的链条，使得根节点位于最左边。因为SentimentModel不处理一元节点，因此该方法可以简化，来构造二叉树。返回新的树和新的标签。原始树不改变。</p>

<p><em>代码？</em></p>

<h4>2.2 Evaluate.java</h4>

<h4>2.3 ReadSentimentDataset.java</h4>

<h3>3. edu.stanford.nlp.rnn</h3>

<h4>3.1 RNNCoreAnnotations.java</h4>

	  
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-28T00:00:00+08:00" itemprop="datePublished"><a href="/%E7%AC%94%E8%AE%B0/HeadFirstJava-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">October 28, 2013</a></time></span><span class="author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name" class="fn"><a href="/about" title="About Ni Peng" itemprop="url">Ni Peng</a></span></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title" itemprop="name"><a href="/%E7%AC%94%E8%AE%B0/HeadFirstJava-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0" rel="bookmark" title="Headfirstjava 学习笔记" itemprop="url">Headfirstjava 学习笔记</a></h1>
    
  </header>
  <div class="entry-content" itemprop="description">
	  
			<h4>1. 基本概念</h4>

<ol>
<li><p>java中integer和boolean两种类型不相容，因此不能写</p>

<pre><code> int x = 1;
 while (x) {}
</code></pre></li>
<li><p>println会在最后插入换行，print不会.</p></li>
<li>String [] lst = ["a", "b"]; lst.length;</li>
<li>Math.random() 随机产生[0,1]之间的小数</li>
<li>int x = (int) 24.6;</li>
<li>+: 连接字符串</li>
<li>面向对象优点：有时不需要改动已经测试号的程序。</li>
<li><p>创建对象需要两个类：一个是被操作的类（比如Dog, AlarmClock等），另一个是测试该类的类。测试用的类带有main()，在其中建立存取被测对象。</p>

<pre><code> class Dog{}
 class DogTestDrive{}
</code></pre></li>
<li><p>java主动管理内存</p></li>
<li>全局变量：任何变量只要加上public，static, final，基本上都会变成全局变量取用的常数。</li>
<li>数量庞大的个别文件，可以pkzip存档为Java Archive-.jar文件。在jar文件中可以引入一个简单的文件，manifest，里面定义jar中哪个文件带有应用程序的main()方法。</li>
<li>所有的java程序都定义在类中。</li>
<li>创建类时，同时创建独立的测试用的类。</li>
<li>Java很注重类型。必须声明所有变量的类型。</li>
<li>变量有两种：primitive主数据类型用来保存基本类型的值，包括整数，布尔，浮点数; 对象引用保存的是对象的引用（？？）</li>
<li>变量必须要有类型，必须要有名称。</li>
<li><p>primitive主数据类型：</p>

<pre><code>布尔：boolean true/false
char: 16 bits 
整型：byte（8位二进制）short(16位) int(32位) long（64位）
浮点：float(32位) double(64位)  
</code></pre></li>
<li>float f = 32.5f;除非加上"f"，否则小数都会被java当作double处理。</li>
<li>确保变量能存下保存的值。</li>
<li>命名规则：

<ul>
<li>名称必须以字母，下划线，$开头，不能以数字开头</li>
<li>除了第一个字符外，后面就可以用数字，反正不要用在第一个字符就行。</li>
<li>避开Java保留字。</li>
</ul>
</li>
<li>对primitive主数据类型的变量来说，变量值就是所代表的值。对<code>引用</code>变量来说，变量值是取得特定对象的位表示法。</li>
<li>引用大小未知；对任意一个Java虚拟机来说，所有的引用大小都一样；不能对引用变量运算。</li>
<li>primitive主数据类型变量值是该值的字节所表示的；引用变量的值代表位于堆之对象的存取方法。引用变量如同遥控器。没有引用到任何对象的引用变量的值位null值。</li>
<li>数组一定是对象。</li>
<li>类描述的是对象知道什么与执行什么</li>
<li>方法运用形参，调用方传入实参</li>
<li>java是通过值传递的，通过拷贝传递</li>
<li>可以忽略方法的返回值</li>
<li>如何隐藏数据？使用公有，私有修饰符。将实例变量标记为私有，提供公有gettet,setter控制存取。</li>
<li>实例变量永远都有默认值:整数0，浮点0.0，布尔false，引用变量null</li>
<li>null表示没有操作对象的远程控制，是引用不是对象</li>
<li>实例变量声明在类内，而不是方法内。局部变量声明在方法中。局部变量在使用前必须初始化，没有默认值。</li>
<li><p>equals和==的区别</p>

<ul>
<li>当参数引用的对象与当前对象为同一对象时，“==“ 和 ”equals” 均为true。</li>
<li><p>如果两个对象的类型一致，并且内容一致，则“equals”返回true,这些类有：
java.io.file,java.util.Date,java.lang.string,包装类（Integer,Double等）</p>

<pre><code>    Animal  animal1=new Dog();
    Animal  animal2=new  Cat();
    Animal animal3=animal1;
    则animal1==animal2   (FALSE)
    animal1.equals(animal2)  (false)
    animal1==animal3   (true)
    animal1.equals(animal3)   (true)

    Integer int1=new Integer(1);
    Integer int2=new Integer(1);
    String str1=new String("hello");
    String str2=new String("hello");
    int1==int2   输出：false,因为不同对象
    int1.equals(int2)   输出：TRUE
    str1==str2   (false)
    str1.equals(str2)   (true)
</code></pre></li>
</ul>
</li>
<li><p>ArrayList.remove</p>

<pre><code>remove(int index) 移除下标为index的元素，返回移除的元素。
remove(object o) 移除元素o,返回true/false
</code></pre></li>
<li>伪码--测试码--真实码</li>
<li>伪代码记录要做什么，而不是怎么做</li>
<li>开始编写代码之前，先学出测试方法用的代码。</li>
<li>极限编程（XP）方法论</li>
<li>思考与编写测试代码有助于了解被测试程序应该要做哪些事情。</li>
<li>Integer.parseInt("3");只对string为数字是有用。</li>
<li>for(int cell: locationCells){}</li>
<li>x++; ++x;</li>
<li>int randomNum = (int)(Math.random()*8);</li>
<li>ArrayList：Java函数库的一个类，可变的。</li>
<li><p>ArrayList无法保存primitive主数据类型。</p>

<p>不能保存int, float, 等primitive等数据类型，不过可以使用Integer等。</p></li>
<li><p>布尔表达式：且（&amp;&amp;）或(||)非（!）</p></li>
<li>&amp;,| 与&amp;&amp;, ||的区别？&amp;和| 是可以用做逻辑运算也可以用做位运算。

<ul>
<li>运算数据类型支持不同：&amp;&amp;,||只支持布尔类型运算；&amp;,|可以支持int, boolean,char三种类型。</li>
<li>逻辑运算不同：&amp;&amp;,||条件运算符;&amp;，|无条件运算符。</li>
</ul>


<p>计算p1&amp;&amp;p2时,Java先计算p1,若p1为true再计算p2;若p1为false,则不再计算P2,因此&amp;&amp;又称为条件与运算符.而&amp;的两个运算对象都要计算,所以,&amp;又称为无条件与运算符.类似的还有 "|| " (条件或运算符,p1 || p2,Java先计算p1,若p1为FALSE再计算P2,若P1为TRUE,则不再计算P2)和 "| " (无条件运算符,两边对象都要计算) .</p>

<p>例如:(a &lt;2)&amp;(b-- &lt;2) 保证(b-- &lt;2)能被计算.这样,无论a是否小于2,变量b都要减1.</p>

<p>要避免使用&amp;和|运算符,它们好处不大.使用&amp;和|运算符会使程序可读性降低,并且可能导致错误,比如:(x!=0)&amp;(100/x)当x为0时产生运行错误,而(x!=0)&amp;&amp;(100/x)没问题</p></li>
<li>java程序不会因为import而变大变慢</li>
<li>除了java.lang这个包里的类，要用到其他的类都要指定完整名称。</li>
</ol>


<h4>7. 继承与多态</h4>

<ol>
<li>继承和包含的关系：is-a测试。</li>
<li>super 可指定使用父类的方法。</li>
<li>父类通过存取权限限制子类是否可继承某些特定成员。

<ul>
<li>private: 不可继承</li>
<li>default</li>
<li>protected</li>
<li>public : 可继承</li>
</ul>
</li>
<li>滥用继承了吗，什么时候使用继承

<ul>
<li>当某个子类比父类更有特定意义时</li>
<li>当行为程序应该被多个相同基本类型所共享时</li>
<li>is-a测试通过</li>
</ul>
</li>
<li>子类是extends父类出来的</li>
<li>子类会继承父类所有public类型的实例变量和方法，但不继承所有的private类型的变量方法</li>
<li>继承下来的方法可以被覆盖，但实例变量不能被覆盖，因为实例变量不能定义行为。</li>
<li>is-a测试是单向的，具有传递性。</li>
<li>java只是由一堆类组成的。</li>
<li>多态性：发送消息给某个对象，让该对象自行决定响应何种行为。

<ul>
<li>通过将子类对象引用赋值给超类对象引用变量来实现动态方法调用。</li>
<li>java 的这种机制遵循一个原则：当超类对象引用变量引用子类对象时，<strong>被引用对象的类型</strong>而不是引用变量的类型决定了调用谁的成员方法，但是这个被调用的方法必须是在超类中定义过的，也就是说被子类覆盖的方法。</li>
</ul>
</li>
<li>方法的重写<strong>Overriding</strong>和重载<strong>Overloading</strong>是Java多态性的不同表现。重写Overriding是父类与子类之间多态性的一种表现，重载Overloading是一个类中多态性的一种表现。如果在子类中定义某方法与其父类有相同的名称和参数，我们说该方法被重写(Overriding)。子类的对象使用这个方法时，将调用子类中的定义，对它而言，父类中的定义如同被“屏蔽”了。如果在一个类中定义了多个同名的方法，它们或有不同的参数个数或有不同的参数类型，则称为方法的重载(Overloading)。Overloaded的方法是可以改变返回值的类型。</li>
<li><p>实际上这里涉及方法调用的优先问题 ，优先级由高到低依次为：</p>

<pre><code>this.show(o) &gt; super.show(o) &gt;
this.show((super)o) &gt; super.show((super)o)
</code></pre></li>
<li>子类层次限制：一般不超过一，两层。也有例外，特别是GUI类这边。</li>
<li>没有私有类概念，不过有办法防止某个类被作出子类：

<ul>
<li>存取控制：类不标注为公有。非公有类智能被同一个包的类作出子类。</li>
<li>final修饰符：表示为继承树的末端，不能被继承。</li>
<li>让类只拥有private的构造程序。</li>
</ul>
</li>
<li>防止特定方法被覆盖，将该方法标注为final。将整个类标注为final，表示没有任何方法被覆盖。</li>
<li>方法是合约的标志。</li>
<li>覆盖的方法参数必须一样，且返回类型必须兼容。</li>
<li>子类覆盖父类方法，覆盖规则：

<ul>
<li>参数必须一样，且返回类型要兼容。参数不一样则不是覆盖，是overload</li>
<li>不能降低方法的存取权限。父类是public，子类不能是private。</li>
</ul>
</li>
<li>重载(overload)的意义是两个方法，名称相同，参数不同。<strong>重载和继承，多态毫无关系</strong>。重载方法和覆盖方法不一样。

<ul>
<li>重载返回类型可以不同。</li>
<li>不能只是返回类型不同而参数相同。</li>
<li>可以修改存取权限。</li>
</ul>
</li>
</ol>


<h4>8.接口与抽象类</h4>

<ol>
<li><p>abstract: 通过标记类位abstract，则不管在哪里，这个类就不能创建任何类型的实例。</p>

<pre><code> abstract class Canine extends Animal{
     public void roam() {}
 }
</code></pre></li>
<li>抽象类除了被继承过之外，没有用途，没有值，没有目的。</li>
<li>抽象类表示此类必须被extend过，抽象的方法表示该方法一定被覆盖过override</li>
<li>抽象的方法没有实体</li>
<li>如果声明了一个抽象的方法，必须将类标记为抽象类。不能在非抽象类中拥有抽象方法。</li>
<li>抽象方法的意义：

<ul>
<li>就算无法实现出方法的内容，还是可以定义出一组子型共同的协议。</li>
<li>支持多态</li>
</ul>
</li>
<li>抽象方法没有内容，只是为了标记出多态存在。</li>
<li>必须实现所有的抽象的方法。</li>
<li></li>
</ol>


<h4>9.构造器与垃圾收集器</h4>

<h4>10.数字局静态</h4>

<h4>11.异常处理</h4>

<h4>12.图形用户接口</h4>

<h4>13.Swing</h4>

<h4>14.序列化和文件的输入输出</h4>

<h4>15.网络与线程</h4>

<h4>16.集合与泛型</h4>

<h4>17.包，jar存档文件和部署</h4>

<h4>18.远程部署RMI</h4>

<h3>书籍推荐</h3>

<ol>
<li>《Head First 设计模式》</li>
<li>《极限编程》</li>
</ol>


	  
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-18T00:00:00+08:00" itemprop="datePublished"><a href="/%E7%AC%94%E8%AE%B0/DL-for-NLP">October 18, 2013</a></time></span><span class="author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name" class="fn"><a href="/about" title="About Ni Peng" itemprop="url">Ni Peng</a></span></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title" itemprop="name"><a href="/%E7%AC%94%E8%AE%B0/DL-for-NLP" rel="bookmark" title="Dl For Nlp" itemprop="url">Dl For Nlp</a></h1>
    
  </header>
  <div class="entry-content" itemprop="description">
	  
			<h2>Deep Learning for NLP(without Magic)笔记</h2>

<h3>0.背景简介</h3>

<hr />

<p>目前机器学习基于人工设计的特征，机器学习只是优化参数权重已达到好的预测效果。</p>

<p>表示学习：尝试自动学习好的特征来表示数据</p>

<p>深度学习：尝试学习多层的表示，以增加复杂度和抽象度。</p>

<p>研究深度学习的五个原因：</p>

<ol>
<li><p>表示性学习：手工设计特征复杂耗时</p></li>
<li><p>distributed representation的必要性：NLP中原子表示法的脆弱性。</p>

<p>基于聚类的Distribional similarity效果很好：</p>

<ul>
<li><p>语法分析 <strong>Brown clustering</strong></p></li>
<li><p>实体识别 <strong>Standford NER, exchange clustering</strong></p></li>
</ul>


<p>Distributed representations可处理维数灾难。</p>

<p>解决方案：</p>

<ul>
<li>手工设计特征</li>
<li>假设光滑目标函数</li>
<li>核函数方法</li>
</ul>
</li>
<li><p>非监督的特征和权重学习
现在，多数NLP/ML方法需要标记好的训练数据，不过大部分数据都是未标注的。因此需要非监督的学习。</p></li>
<li><p>学习多层表示
我们需要有用的中间层表示。We need composi>onality in our ML models.
循环/递归：相同的算子应用在不同的部分上。</p></li>
<li><p>为什么现在？
 2006年之前，deep architectures没有成功。
 改变：</p>

<ul>
<li>提出了关于非监督的预训练方法 <strong>RBM, autoencoders, contrastive estimation</strong></li>
<li>更有效的参数估计方法</li>
<li>更好的理解模型正则化</li>
</ul>
</li>
</ol>


<p><strong>DL取得很好的效果</strong>：</p>

<ul>
<li>Neural Language Model</li>
<li>SENNA POS（词性分析） NER（实体识别）</li>
<li>多核CPU和GPU使得计算速度大幅提升</li>
</ul>


<h3>1.基础知识</h3>

<hr />

<h4>1.1 Motivations</h4>

<h4>1.2 从逻辑回归到神经网络</h4>

<h5>单神经元</h5>

<ul>
<li>n个输入</li>
<li>1个输出</li>
<li>偏置单元</li>
<li>激活函数</li>
<li>参数W,b</li>
</ul>


<h5>从Maxent分类器到神经网络</h5>

<p><img src="/1.png" alt="image" /></p>

<h5>单个神经元计算：</h5>

<pre><code>h_wb(x) = f(wx+b)
f(z) = 1/(1+e^(-z))
</code></pre>

<h5>神经网络=同时运行多个逻辑回归</h5>

<p>训练W:</p>

<ul>
<li>对有监督的单层神经网络，我们可以和训练最大熵模型一样，通过梯度来训练。

<ul>
<li>SGD</li>
<li>对偶梯度(Conjugate gradient 或 L-BFGS)</li>
</ul>
</li>
</ul>


<blockquote><pre><code>  问题1：对偶梯度(Conjugate gradient 或 L-BFGS
</code></pre></blockquote>

<ul>
<li>多层网络较复杂，因为中间隐层逻辑单元使得方程非凸，就像hidden CRFS。不过我们可以应用相同的思路和方法，后向传播算法。</li>
</ul>


<p>为什么需要非线性：</p>

<ul>
<li>对逻辑回归来说，映射到概率。</li>
<li>函数逼近，比如回归，分类。

<ul>
<li>如果没有非线性，深层神经网络并不能比线性变换做更多的事情。因为多层线性总可以转换为复杂的单层线性变换。</li>
<li>除了在Boltzmann机/图模型下，概率理解并不是必须的。人们经常用其他非线性变换，比如tanh。</li>
</ul>
</li>
</ul>


<p>基本概念总结：</p>

<ul>
<li>神经元 = 逻辑回归或相似函数</li>
<li>输入层 = 输入 训练/测试 向量</li>
<li>偏置单元 = 截距项</li>
<li>激活 = 反应</li>
<li>激活函数 是 logistic函数，或其他sigmoid非线性函数</li>
<li>后向传播 =  应用在多层网络上的随机梯度下降</li>
<li>权重衰退 = 正则化/Bayesian先验</li>
</ul>


<p>非监督预训练使得有效的深度学习变为可能</p>

<h4>1.3 词表示(Wordrepresentations)</h4>

<p><strong>The standard word representation</strong></p>

<p>向量空间模型中，向量由一个1和很多0组成：</p>

<blockquote><p>[0, 0, …, 0, 1, 0, …, 0]</p></blockquote>

<p>维数：20k(Speech)-50k(PTB)-500k(big vocab)-13M(Google 1T)</p>

<p>称作“one-hot”representation。</p>

<p>存在的问题：每个词被孤立起来，比如motel和hotel相似度为0.</p>

<p><strong>Distributional similarity based representations</strong></p>

<p><strong>Class-based (hard) and soft clustering word representations</strong></p>

<ul>
<li>Brown clustering (Brown et al. 1992)<em>  Exchange clustering (Clark 2003)</em>  Desparsification and great example of unsupervised pre-training</li>
</ul>


<p>软分类模型学习了在每一类中都有一个单词分布:</p>

<ul>
<li>LSA</li>
<li>LDA, HMM</li>
</ul>


<p><strong>Neural word embeddings as a distributed representation</strong></p>

<blockquote><p>(Bengio et al. 2003, Collobert &amp; Weston 2008, Turian et al. 2010)</p></blockquote>

<p>单词被表示为一个紧致的向量。</p>

<p><strong>Advantages of the neural word embedding approach</strong></p>

<p>neural word embeddings通过增加有监督过程更有意义。</p>

<h4>1.4 非监督词向量的学习</h4>

<p><strong>A neural network for learning word vectors (Collobert et al. JMLR 2011)</strong></p>

<p>思路：一个单词和文本作为正样本；一个随机的单词在相同的文本中作为负样本。</p>

<p>实现：</p>

<blockquote><p>score(cat chills on a mat) > score(cat chills Jeju a mat)</p></blockquote>

<p>怎样计算得分：</p>

<ul>
<li>应用神经网络</li>
<li>每个词表示为n维向量</li>
</ul>


<p><strong>Word embedding matrix</strong></p>

<p>随机初始化所有的词向量，组成word embedding matrix L: n*|V|</p>

<p>从L中得到每个单词的向量：x = Le, 其中e为one-hot向量，表示单词表|V|中第i个单词。</p>

<p><strong>计算得分</strong>
score(cat chills in a mat):</p>

<ul>
<li>表示短语：从L中得到每个单词的表示，cat: n*1, chills: n*1, …</li>
<li>连接所有单词，组成5*n的向量x: 5n*1</li>
</ul>


<p>3层神经网络：</p>

<p>s = score(cat chills on a mat)</p>

<pre><code>s = U^T*f(Wx+b), x:20,1 W:8,20 U:8,1
s = U^T*a 
a = f(z)
z = Wx+b
x = [x_cat x_chills x_on x_a x_mat]
L: n*|V|  这里n=4
</code></pre>

<p>s_c = score(cat chills Jeju a mat)</p>

<p><strong>目标函数</strong></p>

<p>最大化s,最小化s_c，即最小化下式：</p>

<pre><code>J = max(0, 1 - s + s_c)
</code></pre>

<blockquote><pre><code>  很奇怪，为什么要定义上式，而不是max(s - s_c)
</code></pre></blockquote>

<p>它是连续的，因此可以应用SGD。</p>

<p>假设损失J>0，我们可以计算s, s_c关于变量:U, W, b, x的偏导数</p>

<pre><code>∂s/∂U = ∂(U^T*a)/∂U
∂s/∂U = a
</code></pre>

<p><strong>应用后向传播训练</strong></p>

<pre><code>∂s/∂W = ∂(U^T*a)/∂W = ∂/∂W(U^T*f(z)) = ∂/∂W(U^T*f(Wx+b))
</code></pre>

<p>W_ij只出现在a_i中，例如：W_23只用来计算a_2</p>

<pre><code>∂/∂W_ij(U^T*a) = ∂/∂W_ij(U_i*a_i)
∂/∂W_ij(U_i*a_i) = U_i * ∂a_i/∂W_ij
                 = U_i * ∂a_i/∂z_i * ∂z_i/W_ij
                 = U_i * f'(z_i) * ∂z_i/W_ij
                 = U_i * f'(z_i) * ∂/W_ij(W_i*x+b_i)
                 = U_i * f'(z_i) * ∂/W_ij(∑W_ik*x_k)
                 = U_i * f'(z_i) * x_j
                 = delta_i * x_j
delta_i = U_i * f'(z_i) 称作‘局部误差信号’
x_j 称作‘局部输入信号’
</code></pre>

<p>从W_ij 到W:</p>

<pre><code>∂J/∂W = delta * x^T
</code></pre>

<p>对偏置单元b:</p>

<pre><code>∂/∂b_i(U_i*a_i) = U_i * ∂a_i/∂b_i 
                = U_i * f'(z_i) * ∂(W_i*x+b_i)/∂b_i
                = delta_i

∂s/∂x_j = … = delta^T * W_.j
</code></pre>

<h4>1.5 后向传播训练</h4>

<p>简单的链式法则：</p>

<pre><code>z = f(y); y = g(x); ∂z/∂x = ∂z/∂y * ∂y/∂x
∆z = ∂z/∂y * ∆y; ∆y = ∂y/∂x * ∆x; ∆z = ∂z/∂y * ∂y/∂x * ∆x
</code></pre>

<p>多路径链式法则：</p>

<pre><code>z = f(y_1, y_2, …, y_n)
∂z/∂x = ∑∂z/∂y_i * ∂y_i/∂x
</code></pre>

<p>Chain Rule in Flow Graph:</p>

<p>Flow Graph: 任意有向非循环图</p>

<ol>
<li><p>前向传播</p></li>
<li><p>后向传播</p></li>
</ol>


<h4>1.6 学习词级别的分类器：POS（词性标注）, NER(实体识别)</h4>

<p><strong>模型</strong></p>

<pre><code>(Collobert &amp; Weston 2008; Collobert et al. 2011)
</code></pre>

<ul>
<li><p>类似词向量学习过程，把单个得分替换为Softmax/Maxent 分类器</p></li>
<li><p>类似于词向量模型中，通过后向传播训练。</p></li>
</ul>


<h4>1.7 Sharing statistical strength</h4>

<p><strong>Auto-encoders</strong></p>

<p>多层神经网络，目标：output = input</p>

<p>重构 = decoder(encoder(input))</p>

<pre><code>a = tanh(Wx + b)
x' = tanh(W^T*x + b)
cost = ||x' - x||^2
</code></pre>

<p>目标： 使得重构误差最小</p>

<p><strong>PCA</strong></p>

<p>PCA = Linear Manifold = Linear Auto-Encoder</p>

<p><strong>自编码学习凸变量，就像非线性的PCA</strong></p>

<p><strong>Auto-Encoder Variants</strong></p>

<p>离散输入：交叉熵或log-likelihood重构标准</p>

<p>Preventing them to learn the identity everywhere:</p>

<ul>
<li><p>Undercomplete(eg PCA): boCleneck code smaller than input</p></li>
<li><p>Sparsity: penalize hidden unit ac>va>ons so at or near 0</p></li>
<li><p>Denoising: predict true input from corrupted input</p></li>
<li><p>Contractive: force encoder to have small deriva>ves</p></li>
</ul>


<p><strong>Stacking Auto-Encoders</strong></p>

<p>将多个自编码器堆积在一起，组成更复杂的非线性表示。</p>

<p><strong>逐层非监督训练</strong></p>

<h3>2.Recursive Neural Networks(递归神经网络)</h3>

<hr />

<p><strong>怎样将短语映射到向量空间中？</strong></p>

<p>句子的含义由以下决定：</p>

<ol>
<li>每个词的含义</li>
<li>词之间的组织形式</li>
</ol>


<p>递归神经网络可以同时学习组合式向量表示和语法树。</p>

<h4>2.1 Motivation</h4>

<h4>2.2 Recursive Neural Networks for Parsing语法树：</h4>

<table>
<thead>
<tr>
<th>符号 </th>
<th> 含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>S   </td>
<td>句子</td>
</tr>
<tr>
<td>NP  </td>
<td>名词短语</td>
</tr>
<tr>
<td>VP  </td>
<td>动词短语</td>
</tr>
<tr>
<td>PP  </td>
<td>介词短语</td>
</tr>
<tr>
<td>CP  </td>
<td>动补词组</td>
</tr>
<tr>
<td>PP  </td>
<td>介词短语</td>
</tr>
<tr>
<td>PP  </td>
<td>介词短语</td>
</tr>
</tbody>
</table>


<hr />

<table>
<thead>
<tr>
<th>规则    </th>
<th> 含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>S→NP VP </td>
<td>表示“句子”由“名词短语 动词短语”组成</td>
</tr>
<tr>
<td>NP→Det N </td>
<td>表示“名词短语”由“冠词 名词”组成</td>
</tr>
<tr>
<td>VP→VP PP </td>
<td>表示“动词短语”由“动词短语 介词短语”组成</td>
</tr>
<tr>
<td>VP→V NP</td>
<td>表示“动词短语”由“动词 名词短语”组成</td>
</tr>
<tr>
<td>PP→Prep NP</td>
<td> 表示“介词短语”由“介词 名词短语”组成</td>
</tr>
<tr>
<td>Det→the/a </td>
<td> 表示“冠词”由the 或 a 组成</td>
</tr>
<tr>
<td>N→girl/letter/pencil </td>
<td> 表示“名词”由girl/letter/pencil</td>
</tr>
<tr>
<td>V→write </td>
<td>表示“动词”由write组成</td>
</tr>
<tr>
<td>Prep→with</td>
<td>表示“介词”由with组成</td>
</tr>
</tbody>
</table>


<p><strong>递归神经网络用语结构化预测</strong></p>

<ul>
<li>输入：两个候选表示</li>
<li>输出：父节点表示p；父节点的可信度score=U<sup>T</sup>*p</li>
</ul>


<p>应用RNN分解句子：</p>

<pre><code>    x = (x1, …, xn)
</code></pre>

<ol>
<li>分别计算score(x1, x2), …, score(x_n-1, x_n)</li>
<li><p>取分数最大者合并，比如 （x1, x_2）--> x_1'</p>

<pre><code> x = (x_1', x3, …, x_n)
</code></pre></li>
<li><p>重复上述两个步骤，直至合并完成。</p></li>
</ol>


<p><strong>Max-Margin Framework-Details</strong></p>

<h4>2.3 Theory: Backpropagation Through Structure</h4>

<p><strong>Backpropagation Through Structure BTS</strong></p>

<p><strong>Scene Parsing</strong></p>

<p>原理：组合性</p>

<p>算法：Same Recursive Neural Network as for natural language parsing! (Socher et al. ICML 2011)</p>

<p>多类别切分</p>

<h4>2.4 Recursive Autoencoders(RAE)<strong>递归自编码器:</strong></h4>

<p>取代有监督的打分，我们在每个节点上计算重构误差：
    E_rec([c1;c2]) = 1/2||[c1;c2] - [c1';c2']||^2
<strong>半监督递归自编码器:</strong><em> 为了得到情感分析和反义词问题，增加softmax分类器</em> 误差：重构误差和cross-entropy的加权平均<strong>情感检测:</strong>* 多数方法基于<code>词袋模型</code>+<code>语言学特征/处理/词典</code>。</p>

<ul>
<li>上述方法不能分辨出：    * 正面(+) white blood cells destroying an infection

<ul>
<li>负面(-) an infection destroying white blood cells

<h4>2.5 应用到情感分析和Paraphrase Detection</h4></li>
</ul>
</li>
</ul>


<p><strong>怎样比较两句话的含义？</strong></p>

<p>Unsupervised Unfolding RAE and a pair-wise sentence comparison of nodes in parsed trees， Socher et al. (NIPS 2011)</p>

<h4>2.6 Compositionality Through Recursive Matrix-VectorSpaces</h4>

<p>Recursive Matrix-Vector Model：</p>

<pre><code>p = tanh(W[c1 c2]^T + b)
==&gt;
p = tanh(W[C2c1 C1c2]^T + b)
</code></pre>

<h4>2.7 Relation classification</h4>

<p><strong>MV-RNN (Matrix Vector RNN)</strong></p>

<pre><code>www.socher.org
</code></pre>

<h3>3.应用，讨论，相关资料</h3>

<hr />

<p><strong>已存在的NLP应用:</strong></p>

<ul>
<li>语言模型

<ul>
<li>语音识别</li>
<li>机器翻译</li>
</ul>
</li>
<li>词性标注（Part-of-Speech Tagging）</li>
<li>chunking ??</li>
<li>实体识别(Named Entity Recognition)</li>
<li>语义角色标注(Semantic Role Labeling)</li>
<li>情感分析(Sentiment Analysis)</li>
<li>Paraphrasing ??</li>
<li>问答系统(Question-Answering)</li>
<li>语义消歧(Word-Sense Disambiguation)</li>
</ul>


<h4>3.1 应用</h4>

<h5>3.1.1 自然语言模型</h5>

<ul>
<li>预测P(next word|previous word)</li>
<li>计算长句子的概率</li>
<li>应用到语音，翻译，压缩</li>
<li>计算瓶颈：大词表V意味着计算输出：#隐含层*|V|</li>
</ul>


<p><strong>Neural Language Model:</strong></p>

<pre><code>Bengio “A Neural Probabilistic Language Model”
</code></pre>

<ul>
<li>每个词表示为连续值的分布</li>
<li>Generalizes to sequences of words that are semantically similar to training sequences</li>
</ul>


<p><strong>Recurrent Neural Net Language Modeling for ASR</strong></p>

<pre><code>Mikolov et al 2011
http://www.fit.vutbr.cz/~imikolov/rnnlm 
</code></pre>

<p><strong>Neural Net Language Modeling for ASR</strong></p>

<pre><code>Schwenk 2007
</code></pre>

<p><strong>应用到统计机器翻译</strong></p>

<pre><code>http://lium.univ-lemans.fr/cslm/
</code></pre>

<h5>3.1.2 Structured embedding of knowledge bases</h5>

<ol>
<li>Learning Structured Embeddings of Knowledge Bases, (Bordes,Weston, Collobert &amp; Bengio, AAAI 2011）</li>
<li>Joint Learning of Words and Meaning Representations for OpenaText Semantic Parsing, (Bordes,Glorot,Weston &amp; Bengio, AISTATS 2012)</li>
</ol>


<h5>3.1.3 Assorted Speech and NLP Applications</h5>

<p><strong>Learning Multiple Word Vectors</strong></p>

<p>neural word vector</p>

<p><strong>可视化</strong>学习的词向量(Huang et al.(ACL2012))</p>

<p>非监督的预训练（比如DBN, RBM堆积起来），有监督的微调。</p>

<h4>3.2 相关资料(readings, code)</h4>

<p><img src="/2013-10-18-tutorial.png" alt="image" /></p>

<p><img src="/2013-10-18-software.png" alt="image" /></p>

<h4>3.3 Deep Learning Tricks</h4>

<p><strong>“Practical Recommendations for Gradient-Based Training of Deep Architectures” Y. Bengio (2012),</strong></p>

<ul>
<li>非监督预训练</li>
<li>SGD,设置学习速率</li>
<li>主要的超参数

<ul>
<li>Learning rate schedule &amp; Early stopping</li>
<li>Minibatches</li>
<li>参数初始化</li>
<li>隐含层的个数</li>
<li>L1,L2权重衰退</li>
<li>Sparsity regularization</li>
</ul>
</li>
<li>Debugging --> "Finite difference gradient check (Yay)</li>
<li>怎样有效的设置超参数</li>
</ul>


<p><strong>非线性函数</strong></p>

<p><img src="/2013-10-18-log-tanh.png" alt="image" /></p>

<p><strong>tanh 是最常用的，在深度网络中表现最好！</strong></p>

<p><img src="/2013-10-18-other-non-linear.png" alt="image" /></p>

<p><strong>SGD</strong></p>

<ul>
<li>梯度下降一次迭代应用所有的样本</li>
<li>随机梯度下降一次迭代只应用一个样本</li>
<li>传统的梯度下降是batch方法，一次迭代过程非常慢，<code>一般不用</code>.</li>
<li>Use 2<sup>nd</sup> order batch method such as <code>LBFGS</code>.</li>
<li>对大数据集，SGD方法优于所有的batch方法；对小数据集，<code>LBFGS</code>或<code>Conjugat Gradients</code>优于<code>Large-batch LBFGS</code>。</li>
</ul>


<p><strong>Learning Rates</strong></p>

<ul>
<li>简单原则：固定为常数，对所有参数应用相同值</li>
<li>Collobert scales them by the inverse of square root of the fan-in of each neuron</li>
<li>Better results can generally be obtained by allowing learning  rates to decrease, typically in O(1/t) because of theoretical  convergence guarantees, e.g.,</li>
</ul>


<p><img src="/2013-10-18-rate.png" alt="image" /></p>

<p><strong>Long-term dependencies and clipping trick</strong></p>

<ul>
<li>在深度网络中，比如RNN，梯度为一组Jacobian矩阵的乘积。它很容易变得过大或过小，使得梯度下降的局部假设失效。</li>
<li>解决方案是：clip gradients to a maximum value --by Mikolov</li>
</ul>


<p><strong>参数初始化</strong></p>

<ul>
<li>初始化隐含层偏置项为0，输出偏置项为最优值，如果权重0.</li>
<li>初始值~Uniform(-r, r)</li>
<li>r = sqrt(6/(fan-in + fan-out)) for tanh units; 4 * bigger for sigmoid units</li>
</ul>


<p>注：对for embedding weights, fan-in=1 and we don’t care about fan-out, Collobert uses Uniform(-1,1).</p>

<h4>3.4 讨论：局限性，优势，未来研究方向</h4>

	  
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="/page2">2</a>
        
      </li>
    
  </ul>
  
    <a href="/page2" class="btn">Next</a>
  

</div>

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2012-2013 All Rights Reserved. By Ni Peng.

  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>




<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



          

</body>
</html>