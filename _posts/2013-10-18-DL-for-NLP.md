---
layout: post
category : 笔记
tags : [DL, NLP]
---
{% include JB/setup %}

## Deep Learning for NLP(without Magic)笔记

### 0.背景简介
目前机器学习基于人工设计的特征，机器学习只是优化参数权重已达到好的预测效果。

表示学习：尝试自动学习好的特征来表示数据

深度学习：尝试学习多层的表示，以增加复杂度和抽象度。

研究深度学习的五个原因：

1. 表示性学习：手工设计特征复杂耗时

2. distributed representation的必要性：NLP中原子表示法的脆弱性。

   基于聚类的Distribional similarity效果很好：

   * 语法分析 **Brown clustering**

   * 实体识别 **Standford NER, exchange clustering**
   
   Distributed representations可处理维数灾难。
   
   解决方案：
   * 手工设计特征
   * 假设光滑目标函数
   * 核函数方法
   
3. 非监督的特征和权重学习
   现在，多数NLP/ML方法需要标记好的训练数据，不过大部分数据都是未标注的。因此需要非监督的学习。
4. 学习多层表示
   我们需要有用的中间层表示。We need composi>onality in our ML models.
   循环/递归：相同的算子应用在不同的部分上。
5. 为什么现在？
	2006年之前，deep architectures没有成功。
	改变：
	* 提出了关于非监督的预训练方法 **RBM, autoencoders, contrastive estimation**
	* 更有效的参数估计方法
	* 更好的理解模型正则化
	
**DL取得很好的效果**：

*  Neural Language Model
*  SENNA POS（词性分析） NER（实体识别）
*  多核CPU和GPU使得计算速度大幅提升

 