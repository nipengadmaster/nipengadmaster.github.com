---
layout: post
category : 笔记
tags : [DL, NLP]
---
{% include JB/setup %}

## Deep Learning for NLP(without Magic)笔记

### 0.背景简介
---
目前机器学习基于人工设计的特征，机器学习只是优化参数权重已达到好的预测效果。

表示学习：尝试自动学习好的特征来表示数据

深度学习：尝试学习多层的表示，以增加复杂度和抽象度。

研究深度学习的五个原因：

1. 表示性学习：手工设计特征复杂耗时

2. distributed representation的必要性：NLP中原子表示法的脆弱性。

   基于聚类的Distribional similarity效果很好：

   * 语法分析 **Brown clustering**

   * 实体识别 **Standford NER, exchange clustering**

   Distributed representations可处理维数灾难。

   解决方案：
   * 手工设计特征
   * 假设光滑目标函数
   * 核函数方法

3. 非监督的特征和权重学习
   现在，多数NLP/ML方法需要标记好的训练数据，不过大部分数据都是未标注的。因此需要非监督的学习。

4. 学习多层表示
   我们需要有用的中间层表示。We need composi>onality in our ML models.
   循环/递归：相同的算子应用在不同的部分上。

5. 为什么现在？
	2006年之前，deep architectures没有成功。
	改变：
	* 提出了关于非监督的预训练方法 **RBM, autoencoders, contrastive estimation**
	* 更有效的参数估计方法
	* 更好的理解模型正则化

**DL取得很好的效果**：

*  Neural Language Model
*  SENNA POS（词性分析） NER（实体识别）
*  多核CPU和GPU使得计算速度大幅提升

### 1.基础知识
---
#### 1.1 Motivations
#### 1.2 从逻辑回归到神经网络

##### 单神经元

* n个输入
* 1个输出
* 偏置单元
* 激活函数
* 参数W,b

##### 从Maxent分类器到神经网络

![image]({{ site.url}}/images/1.png)

##### 单个神经元计算：

	h_wb(x) = f(wx+b)
	f(z) = 1/(1+e^(-z))

##### 神经网络=同时运行多个逻辑回归

训练W:

* 对有监督的单层神经网络，我们可以和训练最大熵模型一样，通过梯度来训练。
   * SGD
   * 对偶梯度(Conjugate gradient 或 L-BFGS)

> 		问题1：对偶梯度(Conjugate gradient 或 L-BFGS

* 多层网络较复杂，因为中间隐层逻辑单元使得方程非凸，就像hidden CRFS。不过我们可以应用相同的思路和方法，后向传播算法。

为什么需要非线性：

* 对逻辑回归来说，映射到概率。
* 函数逼近，比如回归，分类。
  * 如果没有非线性，深层神经网络并不能比线性变换做更多的事情。因为多层线性总可以转换为复杂的单层线性变换。
  * 除了在Boltzmann机/图模型下，概率理解并不是必须的。人们经常用其他非线性变换，比如tanh。

基本概念总结：

* 神经元 = 逻辑回归或相似函数
* 输入层 = 输入 训练/测试 向量
* 偏置单元 = 截距项
* 激活 = 反应
* 激活函数 是 logistic函数，或其他sigmoid非线性函数
* 后向传播 =  应用在多层网络上的随机梯度下降
* 权重衰退 = 正则化/Bayesian先验

非监督预训练使得有效的深度学习变为可能

#### 1.3 词表示(Wordrepresentations)

**The standard word representation**

向量空间模型中，向量由一个1和很多0组成：
> [0, 0, …, 0, 1, 0, …, 0]

维数：20k(Speech)-50k(PTB)-500k(big vocab)-13M(Google 1T)

称作“one-hot”representation。

存在的问题：每个词被孤立起来，比如motel和hotel相似度为0.

**Distributional similarity based representations**

**Class-based (hard) and soft clustering word representations**

*  Brown clustering (Brown et al. 1992)*  Exchange clustering (Clark 2003)*  Desparsification and great example of unsupervised pre-training

软分类模型学习了在每一类中都有一个单词分布:

* LSA
* LDA, HMM

**Neural word embeddings as a distributed representation**

> (Bengio et al. 2003, Collobert & Weston 2008, Turian et al. 2010)

单词被表示为一个紧致的向量。

**Advantages of the neural word embedding approach**

neural word embeddings通过增加有监督过程更有意义。

#### 1.4 非监督词向量的学习

**A neural network for learning word vectors (Collobert et al. JMLR 2011)**

思路：一个单词和文本作为正样本；一个随机的单词在相同的文本中作为负样本。

实现：

> score(cat chills on a mat) > score(cat chills Jeju a mat)

怎样计算得分：

* 应用神经网络
*  每个词表示为n维向量

**Word embedding matrix**

随机初始化所有的词向量，组成word embedding matrix L: n\*|V|

从L中得到每个单词的向量：x = Le, 其中e为one-hot向量，表示单词表|V|中第i个单词。

**计算得分**
score(cat chills in a mat):

* 表示短语：从L中得到每个单词的表示，cat: n\*1, chills: n\*1, …
* 连接所有单词，组成5\*n的向量x: 5n\*1

3层神经网络：

s = score(cat chills on a mat)

	s = U^T*f(Wx+b), x:20,1 W:8,20 U:8,1
	s = U^T*a
	a = f(z)
	z = Wx+b
	x = [x_cat x_chills x_on x_a x_mat]
	L: n*|V|  这里n=4

s_c = score(cat chills Jeju a mat)

**目标函数**

最大化s,最小化s_c，即最小化下式：

	J = max(0, 1 - s + s_c)

>		很奇怪，为什么要定义上式，而不是max(s - s_c)

它是连续的，因此可以应用SGD。

假设损失J>0，我们可以计算s, s_c关于变量:U, W, b, x的偏导数

	∂s/∂U = ∂(U^T*a)/∂U
	∂s/∂U = a
**应用后向传播训练**

	∂s/∂W = ∂(U^T*a)/∂W = ∂/∂W(U^T*f(z)) = ∂/∂W(U^T*f(Wx+b))

W_ij只出现在a_i中，例如：W_23只用来计算a_2

	∂/∂W_ij(U^T*a) = ∂/∂W_ij(U_i*a_i)
	∂/∂W_ij(U_i*a_i) = U_i * ∂a_i/∂W_ij
	                 = U_i * ∂a_i/∂z_i * ∂z_i/W_ij
	                 = U_i * f'(z_i) * ∂z_i/W_ij
	                 = U_i * f'(z_i) * ∂/W_ij(W_i*x+b_i)
	                 = U_i * f'(z_i) * ∂/W_ij(∑W_ik*x_k)
	                 = U_i * f'(z_i) * x_j
	                 = delta_i * x_j
	delta_i = U_i * f'(z_i) 称作‘局部误差信号’
	x_j 称作‘局部输入信号’

从W_ij 到W:

	∂J/∂W = delta * x^T

对偏置单元b:

	∂/∂b_i(U_i*a_i) = U_i * ∂a_i/∂b_i
	                = U_i * f'(z_i) * ∂(W_i*x+b_i)/∂b_i
	                = delta_i

	∂s/∂x_j = … = delta^T * W_.j

#### 1.5 后向传播训练

简单的链式法则：

	z = f(y); y = g(x); ∂z/∂x = ∂z/∂y * ∂y/∂x
	∆z = ∂z/∂y * ∆y; ∆y = ∂y/∂x * ∆x; ∆z = ∂z/∂y * ∂y/∂x * ∆x

多路径链式法则：

	z = f(y_1, y_2, …, y_n)
	∂z/∂x = ∑∂z/∂y_i * ∂y_i/∂x

Chain Rule in Flow Graph:

Flow Graph: 任意有向非循环图

1. 前向传播

2. 后向传播

#### 1.6 学习词级别的分类器：POS（词性标注）, NER(实体识别)

**模型**

	(Collobert & Weston 2008; Collobert et al. 2011)

* 类似词向量学习过程，把单个得分替换为Softmax/Maxent 分类器

* 类似于词向量模型中，通过后向传播训练。


#### 1.7 Sharing statistical strength

**Auto-encoders**

多层神经网络，目标：output = input

重构 = decoder(encoder(input))

	a = tanh(Wx + b)
	x' = tanh(W^T*x + b)
	cost = ||x' - x||^2


目标： 使得重构误差最小

**PCA**

PCA = Linear Manifold = Linear Auto-Encoder

**自编码学习凸变量，就像非线性的PCA**

**Auto-Encoder Variants**

离散输入：交叉熵或log-likelihood重构标准

Preventing them to learn the identity everywhere:

* Undercomplete(eg PCA): boCleneck code smaller than input

* Sparsity: penalize hidden unit ac>va>ons so at or near 0

* Denoising: predict true input from corrupted input

* Contractive: force encoder to have small deriva>ves

**Stacking Auto-Encoders**

将多个自编码器堆积在一起，组成更复杂的非线性表示。

**逐层非监督训练**

### 2.Recursive Neural Networks(递归神经网络)
---
**怎样将短语映射到向量空间中？**

句子的含义由以下决定：

1. 每个词的含义
2. 词之间的组织形式

递归神经网络可以同时学习组合式向量表示和语法树。

#### 2.1 Motivation
#### 2.2 Recursive Neural Networks for Parsing语法树：

符号 | 含义
----| -------------
S   |句子
NP  |名词短语
VP  |动词短语
PP  |介词短语
CP  |动补词组
PP  |介词短语
PP  |介词短语

--------------------
规则    | 含义
-------|----
S→NP VP |表示“句子”由“名词短语 动词短语”组成
NP→Det N |表示“名词短语”由“冠词 名词”组成
VP→VP PP |表示“动词短语”由“动词短语 介词短语”组成
VP→V NP|表示“动词短语”由“动词 名词短语”组成
PP→Prep NP| 表示“介词短语”由“介词 名词短语”组成
Det→the/a | 表示“冠词”由the 或 a 组成
N→girl/letter/pencil | 表示“名词”由girl/letter/pencil
V→write |表示“动词”由write组成
Prep→with|表示“介词”由with组成

**递归神经网络用语结构化预测**

* 输入：两个候选表示
* 输出：父节点表示p；父节点的可信度score=U^T\*p

应用RNN分解句子：

		x = (x1, …, xn)

1. 分别计算score(x1, x2), …, score(x_n-1, x_n)
2. 取分数最大者合并，比如 （x1, x_2）--> x_1'

		x = (x_1', x3, …, x_n)

3. 重复上述两个步骤，直至合并完成。

**Max-Margin Framework-Details**
#### 2.3 Theory: Backpropagation Through Structure
**Backpropagation Through Structure BTS**

**Scene Parsing**

原理：组合性

算法：Same Recursive Neural Network as for natural language parsing! (Socher et al. ICML 2011)

多类别切分
#### 2.4 Recursive Autoencoders(RAE)**递归自编码器:**
取代有监督的打分，我们在每个节点上计算重构误差：
	E_rec([c1;c2]) = 1/2||[c1;c2] - [c1';c2']||^2
**半监督递归自编码器:*** 为了得到情感分析和反义词问题，增加softmax分类器* 误差：重构误差和cross-entropy的加权平均**情感检测:*** 多数方法基于`词袋模型`+`语言学特征/处理/词典`。

* 上述方法不能分辨出：	* 正面(+) white blood cells destroying an infection
	* 负面(-) an infection destroying white blood cells
#### 2.5 应用到情感分析和Paraphrase Detection

**怎样比较两句话的含义？**

Unsupervised Unfolding RAE and a pair-wise sentence comparison of nodes in parsed trees， Socher et al. (NIPS 2011)

#### 2.6 Compositionality Through Recursive Matrix-VectorSpaces

Recursive Matrix-Vector Model：

	p = tanh(W[c1 c2]^T + b)
	==>
	p = tanh(W[C2c1 C1c2]^T + b)

#### 2.7 Relation classification

**MV-RNN (Matrix Vector RNN)**

	www.socher.org

### 3.应用，讨论，相关资料
---
**已存在的NLP应用:**

* 语言模型
  * 语音识别
  * 机器翻译
* 词性标注（Part-of-Speech Tagging）
* chunking ??
* 实体识别(Named Entity Recognition)
* 语义角色标注(Semantic Role Labeling)
* 情感分析(Sentiment Analysis)
* Paraphrasing ??
* 问答系统(Question-Answering)
* 语义消歧(Word-Sense Disambiguation)

#### 3.1 应用

##### 3.1.1 自然语言模型

* 预测P(next word|previous word)
* 计算长句子的概率
* 应用到语音，翻译，压缩
* 计算瓶颈：大词表V意味着计算输出：#隐含层*|V|

**Neural Language Model:**

	Bengio “A Neural Probabilistic Language Model”

* 每个词表示为连续值的分布
* Generalizes to sequences of words that are semantically similar to training sequences

**Recurrent Neural Net Language Modeling for ASR**

	Mikolov et al 2011
	http://www.fit.vutbr.cz/~imikolov/rnnlm

**Neural Net Language Modeling for ASR**

	Schwenk 2007

**应用到统计机器翻译**

	http://lium.univ-lemans.fr/cslm/

##### 3.1.2 Structured embedding of knowledge bases

1. Learning Structured Embeddings of Knowledge Bases, (Bordes,Weston, Collobert & Bengio, AAAI 2011）
2. Joint Learning of Words and Meaning Representations for OpenaText Semantic Parsing, (Bordes,Glorot,Weston & Bengio, AISTATS 2012)

##### 3.1.3 Assorted Speech and NLP Applications

**Learning Multiple Word Vectors**

neural word vector

**可视化**学习的词向量(Huang et al.(ACL2012))

非监督的预训练（比如DBN, RBM堆积起来），有监督的微调。

#### 3.2 相关资料(readings, code)


![image]({{ site.img_url }}/2013-10-18-tutorial.png)


![image]({{ site.img_url }}/2013-10-18-software.png)

#### 3.3 Deep Learning Tricks

**“Practical Recommendations for Gradient-Based Training of Deep Architectures” Y. Bengio (2012),**

* 非监督预训练
* SGD,设置学习速率
* 主要的超参数
	* Learning rate schedule & Early stopping
	* Minibatches
	* 参数初始化
	* 隐含层的个数
	* L1,L2权重衰退
	* Sparsity regularization
* Debugging --> "Finite difference gradient check (Yay)
* 怎样有效的设置超参数

**非线性函数**

![image]({{ site.img_url }}/2013-10-18-log-tanh.png)

**tanh 是最常用的，在深度网络中表现最好！**

![image]({{ site.img_url }}/2013-10-18-other-non-linear.png)

**SGD**

* 梯度下降一次迭代应用所有的样本
* 随机梯度下降一次迭代只应用一个样本
* 传统的梯度下降是batch方法，一次迭代过程非常慢，`一般不用`.
* Use 2^nd order batch method such as `LBFGS`.
* 对大数据集，SGD方法优于所有的batch方法；对小数据集，`LBFGS`或`Conjugat Gradients`优于`Large-batch LBFGS`。

**Learning Rates**

* 简单原则：固定为常数，对所有参数应用相同值
* Collobert scales them by the inverse of square root of the fan-in of each neuron
* Better results can generally be obtained by allowing learning  rates to decrease, typically in O(1/t) because of theoretical  convergence guarantees, e.g.,

![image]({{ site.img_url }}/2013-10-18-rate.png)

**Long-term dependencies and clipping trick**

* 在深度网络中，比如RNN，梯度为一组Jacobian矩阵的乘积。它很容易变得过大或过小，使得梯度下降的局部假设失效。
* 解决方案是：clip gradients to a maximum value --by Mikolov

**参数初始化**

* 初始化隐含层偏置项为0，输出偏置项为最优值，如果权重0.
* 初始值~Uniform(-r, r)
* r = sqrt(6/(fan-in + fan-out)) for tanh units; 4 * bigger for sigmoid units

注：对for embedding weights, fan-in=1 and we don’t care about fan-out, Collobert uses Uniform(-1,1).

#### 3.4 讨论：局限性，优势，未来研究方向
